{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import cupy as np\n",
    "import multiprocessing\n",
    "import umap\n",
    "import pickle\n",
    "import module1_Preprocessing as module1\n",
    "import module2_FeatureExtraction as module2\n",
    "from functools import partial\n",
    "\n",
    "os.chdir('../')\n",
    "kaggle_path = os.path.join('Raw','Kaggle','mbti_1.csv')\n",
    "reddit_path = os.path.join('Raw','Reddit','mbti9k_comments.csv')\n",
    "reddit_nodup_path = os.path.join('Raw','Reddit','mbti9k_nodup.csv')\n",
    "seed=123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 506 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MBTI</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://www.youtube.com/watch?v=qsXHcwe3krw|||h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ENTP</td>\n",
       "      <td>I'm finding the lack of me in these posts very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>INTP</td>\n",
       "      <td>Good one  _____   https://www.youtube.com/watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>Dear INTP,   I enjoyed our conversation the ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ENTJ</td>\n",
       "      <td>You're fired.|||That's another silly misconcep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MBTI                                               post\n",
       "0  INFJ  http://www.youtube.com/watch?v=qsXHcwe3krw|||h...\n",
       "1  ENTP  I'm finding the lack of me in these posts very...\n",
       "2  INTP  Good one  _____   https://www.youtube.com/watc...\n",
       "3  INTJ  Dear INTP,   I enjoyed our conversation the ot...\n",
       "4  ENTJ  You're fired.|||That's another silly misconcep..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "kaggle = pd.read_csv(kaggle_path,header=0, names=['MBTI','post'])\n",
    "kaggle['post'] = kaggle['post'].apply(lambda x: x.strip(\"'\"))\n",
    "# kaggle_clean=module1.basicpreprocessing(kaggle,'post')\n",
    "kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that the post contains data leakage keywords\n",
    "MBTIcheatList=(([(len(re.findall(r'\\b{}s?\\b|\\b{}s?\\b'.format(df['MBTI'].upper(),df['MBTI'].lower())*i,df['post']))) for i, df in kaggle.iterrows()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAFNCAYAAADRi2EuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5QtZX3m8e8jB+SmArE1COghkeU9ojnBaxwC3nWCmaiRIYgJEV1R44VRMclM1OioiQrmpkFRQQ1CEK8YERVwuUzQgyKCaEA8KnoGjgIBNFHA3/xR1bptu/vs7tP17t19vp+19updb93eqnfv7qer6q1KVSFJkqTh3WbSFZAkSdpeGLwkSZIaMXhJkiQ1YvCSJElqxOAlSZLUiMFLkiSpEYOXNIWSXJrk4MbrTJJ3JLkuyedarluSthcGL2kRSTYl+XGSO84pvyhJJVm/Aut4Z5JXjZZV1X2q6rxtXfYSPRx4FLBvVR3UeN0LSrK+39dfmFN+x75tNo2UbUryn0lu6gPkWUn268f9S19+U5Kb+3lnh9+S5OAkVy1Sj01JHjnYhmpFJDkvyR9Nuh7SQgxe0tZ9Azh8diDJ/YBdJledwdwN2FRVP5hUBZKsW2T0bknuOzL8P+naZq7/XlW7A3sDVwN/C1BVj6uq3ftx7wH+ana4qp69Qpsw1bayf9fceqVpZPCStu5dwNNHho8CThmdIMltk7w+ybeSXN0fQdmlH3dwkquSHJvkmiSbk/xBP+4Y4AjgJf2Rlw/35T89utIv+4Qk3+1fJyS57daWPZ8kd0nyoSTXJrkiyTP78qOBtwEP6evxinnmfXmSd48Mzx6JWtcPPyPJlUluTPKNJEeMTPuHSS7rj0KdneRuI+MqyXOSXA5cvpV2OGpk+Olz22FUVf0XcAZw70WWOZYk7wLuCny43z8v6Y+mPW/OdBcneVL/vpL8Sb9Pvpfkr5PcZmTaBffJnGXO7udj+vbfnOTYkfG3SXJckq8n+X6S05PsNWfeo5N8C/jUPMt/RpLPzCmrJHfv3z8+yVf6dv1Okv81Mt0T0x39vT7JZ5P82si4TUlemuRi4Afzha/F9lG/XX+e5Jv9Z/uUJHfox+2c5N399l6f5PNJ7pzk1cBvAn/Xt9PfLdSm0sRUlS9fvhZ4AZuARwJfA+4F7AB8m+7oUAHr++lOAD4E7AXcDvgw8Jp+3MHALcArgR2BxwM/BPbsx78TeNV86+3fvxL4N+BOwAzwWeAvx1n2PNtzPvAPwM7AgcAW4NB+3DOAzyyyL14OvHtkeH2/D9YBuwE3APfox+0N3Kd//yTgin7/rQP+HPjsyHIKOKffd7vMs97Z9azv9/0O/bK+1rfNpgX2267AycAp8yxzvn1+MHDV1j4LI8NPBS4YGb4/8H1gp5HtOrffrrsC/w780Tj7ZIHtP7Xfz/fr2212O1/Qfz72BW4L/CNw6px5T+nnnW///kK79/PcvX+/GfjN/v2ewAP79w8ErgEe1LfJUf0+uu3I/roI2G++9Y6xj/6w30e/AuwOnAm8qx/3LLrv2K79un8duH0/7rzZZfjyNY0vj3hJ45k96vUo4KvAd2ZHJAnwTOCFVXVtVd0I/F/gaSPz3wy8sqpurqqPAjcB9xhz3Uf0815TVVuAVwBHLnXZ6a51ejjw0qr6r6q6iO4o15Fzp12mnwD3TbJLVW2uqkv78mfRhdDLquoWun1z4JwjPK/p991/LrL8q/hZ2PqFo44jPpDkerog+Cjgr7dhmxbzQeCAJAf0w0cCp1XVj0emeV2/Xd+iC+ezp6zH2SdzvaKqflBVXwbeMWdZf1ZVV1XVj+gC8pPnHGF6eT/vYvt3ITcD905y+6q6rqpmr7V7JvCPVXVBVd1aVScDPwIePDLv31TVt7ey3oX20RHAG6vqyqq6CXgZ8LR+u24GfokuHN5aVRdW1Q3L2DapOYOXNJ530V1T9Ax+8Q/+DN1/3hf2pz2uBz7Wl8/6fv8HdtYP6f6LH8ddgG+ODH+zL1vqsu8CzAbD0WXtM2Y9FlTddWG/Bzwb2NyfhrtnP/puwJtG9s21QOas99tjruoUujY4HHj3AtM8qar2oDv681zg/CS/vJTtGUcfck4Hfr8/PXY43edk1Oh2jbbbOPtkrsWW9f6RZV0G3ArceYF5l+p36Y6kfjPJ+UkeMrLeY2fX2697P37+sznOehfarvk+9+votutdwNnAe/vTr3+VZMelbpg0CQYvaQxV9U26C7kfT3fKY9T3gP+kO7W2R/+6Q3UXcY+1+K2M/y7dH7lZd+3Lluq7wF5JbjdnWd9ZYPq5fkAXMGf9XJipqrOr6lF0pxm/Cry1H/Vt4Fkj+2aPqtqlqj47OvuYdXgf8ATgyr5NFtQfCTmTLoQ8fMzlL7rIecpOpjsycyjww6r61znj9xt5P9pu4+yTuRZb1uPmLGvnqhpt18X278+169yQWlWfr6rD6E51f4AubM6u99Vz1rtrVZ065nq3tl3zfe5vAa7uj+6+oqruDTwUeCI/uw5z3M+SNBEGL2l8RwOH1Jxef1X1E7qQcXySOwEk2SfJY8Zc7tV017Es5FTgz5PMpLutxf9h4aM9C6qqb9NdH/aa/uLkX6PbpveMuYiLgEckuWt/kfPLZkf0Fzb/dpLd6E433UQXeADeArwsyX36ae+Q5ClLrX+/DT8ADgG2eruAdA6juy7psuWsb45faKc+aP0EeAO/eLQL4MVJ9uxP8z4fOK0vX84++d9Jdu3n+YM5y3r17GnK/nNy2BK260vAfZIcmGRnulOV9MvaKckRSe5QVTfTnb6dbde3As9O8qB+X++W5Alzgv04FtpHpwIvTLJ/kt3pTseeVlW3JPmtJPdLskNfp5tH6rW175M0UQYvaUxV9fWq2rjA6JfSXQj8b0luAD7B+NdwnUR3Dc31ST4wz/hXARuBi4EvA1/oy5bjcLoLrr8LvB/4i6o6Z5wZ++lO6+txIfCRkdG3AY7tl3st8N+AP+7nez/wOrrTQjcAlwCPW2b9qaqNVfX1RSb5cJKb6P4gvxo4auR6s23xGroAfP1ozz6605/3Y/4w/EG6fXURcBZdWy93n5xP9xn7JPD6qvp4X/4muo4dH09yI92F9g8ad6Oq6t/pOmd8gq5X6WfmTHIksKmv57OB3+/n20h3ndffAdf1dXvGuOsdMe8+At5OF2Y/TXe0+b+A2V6kv0zXY/UGulB9Pj/b/2+iu8btuiR/s4z6SINKlUdlJWm5kjwdOKaqHj6nvIADquqKbVz+errgseOca/lWvZXaR9Jq4hEvSVqmJLvSHdk7cdJ1kbQ6DB68kuyQ5ItJPtIP75/kgiSXJzktyU5D10GSVlp/Dd8WumuK/mnC1ZG0Sgx+qjHJi4ANdDe3e2KS04Ezq+q9Sd4CfKmq3jxoJSRJkqbAoEe8kuxL1/X7bf1w6HokndFPcjLdHZwlSZLWvKFPNZ4AvISuuzV0dxq+fuQC0atYgZs3SpIkrQaDPTE+yROBa6rqwiQHzxbPM+m85zrTPTz4GIDddtvt1+95z3vON5kkSdJUufDCC79XVTPzjRsseAEPA347yePpHsh7e7ojYHskWdcf9dqXBe7AXVUn0vcU2rBhQ23cuNDtkyRJkqZHkgWfrDHYqcaqellV7VtV6+keFvypqjqC7kn0T+4nO4ru5nmSJElr3iTu4/VS4EVJrqC75uukrUwvSZK0Jgx5qvGnquo84Lz+/ZXAQS3WK0mSNE28c70kSVIjBi9JkqRGDF6SJEmNGLwkSZIaMXhJkiQ1YvCSJElqxOAlSZLUiMFLkiSpEYOXJElSIwYvSZKkRgxekiRJjRi8JEmSGjF4SZIkNWLwkiRJasTgJUmS1IjBS5IkqRGDlyRJUiMGL0mSpEYMXpIkSY0YvCRJkhoxeEmSJDVi8JIkSWrE4CVJktSIwUuSJKkRg5ckSVIjgwWvJDsn+VySLyW5NMkr+vJ3JvlGkov614FD1UGSJGmarBtw2T8CDqmqm5LsCHwmyb/0415cVWcMuG5JkqSpM1jwqqoCbuoHd+xfNdT6JEmSpt2g13gl2SHJRcA1wDlVdUE/6tVJLk5yfJLbDlkHSZKkaTFo8KqqW6vqQGBf4KAk9wVeBtwT+A1gL+Cl882b5JgkG5Ns3LJly5DVlCRJaqJJr8aquh44D3hsVW2uzo+AdwAHLTDPiVW1oao2zMzMtKimJEnSoIbs1TiTZI/+/S7AI4GvJtm7LwvwJOCSoeogSZI0TYbs1bg3cHKSHegC3ulV9ZEkn0oyAwS4CHj2gHWQJEmaGkP2arwYeMA85YcMtU5JkqRp5p3rJUmSGjF4SZIkNWLwkiRJasTgJUmS1IjBS5IkqRGDlyRJUiMGL0mSpEYMXpIkSY0YvCRJkhoxeEmSJDVi8JIkSWrE4CVJktSIwUuSJKkRg5ckSVIjBi9JkqRGDF6SJEmNGLwkSZIaMXhJkiQ1YvCSJElqxOAlSZLUiMFLkiSpEYOXJElSIwYvSZKkRgxekiRJjRi8JEmSGhkseCXZOcnnknwpyaVJXtGX75/kgiSXJzktyU5D1UGSJGmaDHnE60fAIVV1f+BA4LFJHgy8Dji+qg4ArgOOHrAOkiRJU2Ow4FWdm/rBHftXAYcAZ/TlJwNPGqoOkiRJ02TQa7yS7JDkIuAa4Bzg68D1VXVLP8lVwD5D1kGSJGlaDBq8qurWqjoQ2Bc4CLjXfJPNN2+SY5JsTLJxy5YtQ1ZTkiSpiSa9GqvqeuA84MHAHknW9aP2Bb67wDwnVtWGqtowMzPTopqSJEmDGrJX40ySPfr3uwCPBC4DzgWe3E92FPDBoeogSZI0TdZtfZJl2xs4OckOdAHv9Kr6SJKvAO9N8irgi8BJA9ZBkiRpagwWvKrqYuAB85RfSXe9lyRJ0nbFO9dLkiQ1YvCSJElqxOAlSZLUiMFLkiSpEYOXJElSIwYvSZKkRgxekiRJjRi8JEmSGjF4SZIkNWLwkiRJasTgJUmS1IjBS5IkqRGDlyRJUiMGL0mSpEYMXpIkSY0YvCRJkhoxeEmSJDVi8JIkSWrE4CVJktSIwUuSJKkRg5ckSVIjBi9JkqRGDF6SJEmNGLwkSZIaMXhJkiQ1MljwSrJfknOTXJbk0iTP78tfnuQ7SS7qX48fqg6SJEnTZN2Ay74FOLaqvpDkdsCFSc7pxx1fVa8fcN2SJElTZ7DgVVWbgc39+xuTXAbsM9T6JEmSpl2Ta7ySrAceAFzQFz03ycVJ3p5kzxZ1kCRJmrTBg1eS3YH3AS+oqhuANwO/ChxId0TsDQvMd0ySjUk2btmyZehqSpIkDW7Q4JVkR7rQ9Z6qOhOgqq6uqlur6ifAW4GD5pu3qk6sqg1VtWFmZmbIakqSJDUxZK/GACcBl1XVG0fK9x6Z7HeAS4aqgyRJ0jQZslfjw4AjgS8nuagv+1Pg8CQHAgVsAp41YB0kSZKmxpC9Gj8DZJ5RHx1qnZIkSdPMO9dLkiQ1YvCSJElqxOAlSZLUiMFLkiSpEYOXJElSIwYvSZKkRgxekiRJjRi8JEmSGjF4SZIkNWLwktag9cedNekqSJLmYfCSJElqxOAlSZLUiMFLkiSpEYOXJElSIwYvSZKkRgxe0jLZc3DtsU0lDW2s4JXkYeOUSZIkaWHjHvH62zHLJEmStIB1i41M8hDgocBMkheNjLo9sMOQFZMkSVprFg1ewE7A7v10txspvwF48lCVkiRJWosWDV5VdT5wfpJ3VtU3G9VJkiRpTdraEa9Zt01yIrB+dJ6qOmSISkmSJK1F4wavfwbeArwNuHW46kiSlmL9cWex6bVPmHQ1JI1p3OB1S1W9edCaSJIkrXHj3k7iw0n+OMneSfaafQ1aM0mSpDVm3CNeR/U/XzxSVsCvLDRDkv2AU4BfBn4CnFhVb+oD22l014ttAp5aVdctrdqSJEmrz1jBq6r2X8aybwGOraovJLkdcGGSc4BnAJ+sqtcmOQ44DnjpMpYvSZK0qowVvJI8fb7yqjploXmqajOwuX9/Y5LLgH2Aw4CD+8lOBs7D4CVJkrYD455q/I2R9zsDhwJfoDuVuFVJ1gMPAC4A7tyHMqpqc5I7jVtZSZKk1Wysi+ur6nkjr2fShaidxpk3ye7A+4AXVNUN41YsyTFJNibZuGXLlnFnW9PWH3fWqlquJEn6eeP2apzrh8ABW5soyY50oes9VXVmX3x1kr378XsD18w3b1WdWFUbqmrDzMzMMqspSZI0Pca9xuvDdL0YoXs49r2A07cyT4CTgMuq6o0joz5E10vytf3PDy6xzpIkSavSuNd4vX7k/S3AN6vqqq3M8zDgSODLSS7qy/6ULnCdnuRo4FvAU5ZQX0mSpFVr3NtJnJ/kzvzsIvvLx5jnM0AWGH3oeNWTJElaO8a6xivJU4HP0R2deipwQZInD1kxSZKktWbcU41/BvxGVV0DkGQG+ARwxlAVkyRJWmvG7dV4m9nQ1fv+EubVCpv07R8mvX5pe+V3T1r9xj3i9bEkZwOn9sO/B3x0mCpJkiStTYsGryR3p7vT/IuT/A/g4XQXzP8r8J4G9ZMkSVoztna68ATgRoCqOrOqXlRVL6Q72nXC0JWTJElaS7YWvNZX1cVzC6tqI7B+kBpJkiStUVsLXjsvMm6XlayIJEnSWre14PX5JM+cW9jfdf7CYaq0uqxULyN7K22/bHtJ2n5srVfjC4D3JzmCnwWtDcBOwO8MWTFJkqS1ZtHgVVVXAw9N8lvAffvis6rqU4PXTJIkaY0Z91mN5wLnDlwXSZKkNc27z0uSJDVi8JIkSWrE4CVJktSIwUtaAm/9sDrZbpKmhcFLkiSpEYOXJElSIwYvSZKkRgxekiRJjRi8JEmSGjF4SZIkNWLwEtC+u73d+9cm21WSFmfwkiRJasTgJUmS1MhgwSvJ25Nck+SSkbKXJ/lOkov61+OHWr8kSdK0GfKI1zuBx85TfnxVHdi/Pjrg+iVJkqbKYMGrqj4NXDvU8iVJklabSVzj9dwkF/enIvecwPolSZImonXwejPwq8CBwGbgDQtNmOSYJBuTbNyyZUur+klL5i0UJEnjahq8qurqqrq1qn4CvBU4aJFpT6yqDVW1YWZmpl0lJUmSBtI0eCXZe2Twd4BLFppWkiRprVk31IKTnAocDNwxyVXAXwAHJzkQKGAT8Kyh1i9JkjRtBgteVXX4PMUnDbU+SZKkaeed6yVJkhoxeEmSJDVi8FIz3nZB45r9rCz3MzMtn7Vpqce2WuntWCv7RVoOg5ckSVIjBi9JkqRGDF6SJEmNGLwkSZIaMXhJkiQ1YvDSmmJvqbVtOe07zZ+Jaa6bts72Wznz7cu1un8NXpIkSY0YvCRJkhoxeEmSJDVi8JIkSWrE4CVJktSIwUuSJKkRg5c0RdZq92mtDWvp87mWtmUS3H/LZ/CSJElqxOAlSZLUiMFLkiSpEYOXJElSIwYvSZKkRgxekiRJjRi81qj1x521YHffxcatdB30M0PvjyGXb1uubtPeftNYv2ms01JNwzZMqg7TsO0LMXhJkiQ1YvCSJElqZLDgleTtSa5JcslI2V5Jzklyef9zz6HWL0mSNG2GPOL1TuCxc8qOAz5ZVQcAn+yHJUmStguDBa+q+jRw7Zziw4CT+/cnA08aav2SJEnTpvU1Xneuqs0A/c87NV6/JEnSxEztxfVJjkmyMcnGLVu2TLo624Vpu8XEUuszzd2HZ61EHVdyO5e7rLnz2WW8ve11n096/eNa7HY+K7ns1bI/VsqQ+7WV1sHr6iR7A/Q/r1lowqo6sao2VNWGmZmZZhWUJEkaSuvg9SHgqP79UcAHG69fkiRpYoa8ncSpwL8C90hyVZKjgdcCj0pyOfCofliSJGm7sG6oBVfV4QuMOnSodUqSJE2zqb24XpIkaa0xeEmSJDVi8FqlJtF1dqXWuZq6/U67tbwv1/K2LWS1bPO03E5k2qz0flitv3Nn1+fnYn4GL0mSpEYMXpIkSY0YvCRJkhoxeEmSJDVi8JIkSWrE4LXKLPZw1NXc07H1+rc236S3ayVMwzYu52G+Q32u10KbrpRp2RfrjztrauqyFC0eUj3fcqdlf630g6qntTfoUAxekiRJjRi8JEmSGjF4SZIkNWLwkiRJasTgJUmS1IjBS5IkqRGD1xzjdBNerKvquN19W3RHnnZL3U9LnXe56x1indtiqett3TV72h6IOy31mGu5t9NY7nKmxdC/65ayzG35nbOc9S1Vy7Zd7t83bTuDlyRJUiMGL0mSpEYMXpIkSY0YvCRJkhoxeEmSJDVi8JIkSWrE4DWPleoyu1puP7Cty9nW9bXaT2uhK/RSu+YPeduNaTGt9ZzU96LV7USG/vzNt5xx67Kaf4evxDpHb2vU+hYaQ+2zaf2eL4fBS5IkqRGDlyRJUiPrJrHSJJuAG4FbgVuqasMk6iFJktTSRIJX77eq6nsTXL8kSVJTnmqUJElqZFLBq4CPJ7kwyTETqoMkSVJTkwpeD6uqBwKPA56T5BFzJ0hyTJKNSTZu2bKlfQ2nyLZ05R3tVtxSy3WuVPfrccsntU+HNO23Pplvn2/rrTK2tZv9uONa3ZZlqGVNs5XeZ9vLftsWk/x7sq2/j6elfScSvKrqu/3Pa4D3AwfNM82JVbWhqjbMzMy0rqIkSdKKax68kuyW5Haz74FHA5e0rockSVJrk+jVeGfg/Ulm1/9PVfWxCdRDkiSpqebBq6quBO7fer2SJEmT5u0kJEmSGjF4SZIkNWLwWsBKdjVfalfXIbu8Lqfb7VLrs61Psl/KrR2GqNNKW872DH1rgeV+Pre1y/00fLZbL6ullre/WM56hrStv3eWu6zlrGs5yx/3dipL+QxMuq3W+q2OFmLwkiRJasTgJUmS1IjBS5IkqRGDlyRJUiMGL0mSpEYMXiOW29thGnpTLbeXzErVfZp7q43b+2caeruMa4iehSv9WZjGB9cup7fytvZwHmfaofbBUnu4bUtP26X+PllsP09rz9nt0dYeUD1J0/g7ZhwGL0mSpEYMXpIkSY0YvCRJkhoxeEmSJDVi8JIkSWrE4CVJktSIwau30l3Gx51/2rq5LmTcWwSshttTjLvebbm1x0o8BHe5614tVvLBzCthUreT2ZbfPdP8mdnWOi/noezj3A5jqN9R2/r5mbbvw1KNe3uQpd4CYjXui60xeEmSJDVi8JIkSWrE4CVJktSIwUuSJKkRg5ckSVIjBi9JkqRGDF5bsRJdoheafq11kx2nK/dyl7FSXf1b7vPldoOfps/FpG6xsFwrud7FvrNL/T4PfSuDodY5Te0/yc/UStxGZ7HlTHI/T+q2DSvx92Kl5mvN4CVJktSIwUuSJKmRiQSvJI9N8rUkVyQ5bhJ1kCRJaq158EqyA/D3wOOAewOHJ7l363pIkiS1NokjXgcBV1TVlVX1Y+C9wGETqIckSVJTkwhe+wDfHhm+qi+TJEla01JVbVeYPAV4TFX9UT98JHBQVT1vznTHAMf0g/cAvjZw1e4IfG/gdagt23TtsU3XHtt07bFN4W5VNTPfiHWta0J3hGu/keF9ge/OnaiqTgRObFWpJBurakOr9Wl4tunaY5uuPbbp2mObLm4Spxo/DxyQZP8kOwFPAz40gXpIkiQ11fyIV1XdkuS5wNnADsDbq+rS1vWQJElqbRKnGqmqjwIfncS6F9HstKaasU3XHtt07bFN1x7bdBHNL66XJEnaXvnIIEmSpEYMXvgIo9UiyX5Jzk1yWZJLkzy/L98ryTlJLu9/7tmXJ8nf9O16cZIHjizrqH76y5McNaltUifJDkm+mOQj/fD+SS7o2+e0viMOSW7bD1/Rj18/soyX9eVfS/KYyWyJAJLskeSMJF/tv68P8Xu6uiV5Yf9795IkpybZ2e/p8mz3wctHGK0qtwDHVtW9gAcDz+nb6jjgk1V1APDJfhi6Nj2gfx0DvBm6oAb8BfAguicp/MXsHwFNzPOBy0aGXwcc37fpdcDRffnRwHVVdXfg+H46+s/B04D7AI8F/qH/bmsy3gR8rKruCdyfrm39nq5SSfYB/gTYUFX3pesY9zT8ni7Ldh+88BFGq0ZVba6qL/Tvb6T7Zb4PXXud3E92MvCk/v1hwCnV+TdgjyR7A48Bzqmqa6vqOuAcul8CmoAk+wJPAN7WDwc4BDijn2Rum8629RnAof30hwHvraofVdU3gCvovttqLMntgUcAJwFU1Y+r6nr8nq5264BdkqwDdgU24/d0WQxePsJoVeoPXT8AuAC4c1Vthi6cAXfqJ1uobW3z6XIC8BLgJ/3wLwHXV9Ut/fBo+/y07frx/9FPb5tOj18BtgDv6E8fvy3Jbvg9XbWq6jvA64Fv0QWu/wAuxO/pshi8IPOU2dVziiXZHXgf8IKqumGxSecpq0XK1ViSJwLXVNWFo8XzTFpbGWebTo91wAOBN1fVA4Af8LPTivOxTadcf4r3MGB/4C7AbnSniOfyezoGg9eYjzDSdEiyI13oek9VndkXX92fmqD/eU1fvlDb2ubT42HAbyfZRHea/xC6I2B79Kc04Ofb56dt14+/A3Attuk0uQq4qqou6IfPoAtifk9Xr0cC36iqLVV1M3Am8FD8ni6LwctHGK0a/TUCJwGXVdUbR0Z9CJjt8XQU8MGR8qf3vaYeDPxHf4rjbODRSfbs/5N7dF+mxqrqZVW1b1Wtp/vufaqqjgDOBZ7cTza3TWfb+sn99NWXP63vTbU/3YXan2u0GRpRVf8P+HaSe/RFhwJfwe/pavYt4MFJdu1/D8+2qd/TZZjIneuniY8wWlUeBhwJfDnJRX3ZnwKvBU5PcjTdL4in9OM+Cjye7gLOHwJ/AFBV1yb5S7rQDfDKqrq2zSZoTC8F3pvkVcAX6S/U7n++K8kVdP9BPw2gqi5NcjrdH4NbgOdU1a3tq63e84D39P/MXkn33bsNfk9Xpaq6IMkZwBfovl9fpLs7/Vn4PV0y71wvSZLUiKcaJUmSGjF4SZIkNWLwkiRJasTgJUmS1IjBS5IkqRGDl6Q1Icn6JJfMKXt5kv81qTpJ0lwGL0lawMhduSVpRRi8JK15Sf4kyVeSXJzkvX3ZbknenuHNXeMAAAF9SURBVOTz/cOcD+vLn5Hkn5N8GPh4kr2TfDrJRUkuSfKbE90YSaua/81J2h4cB+xfVT9Kskdf9md0jzL5w77sc0k+0Y97CPBr/d3TjwXOrqpXJ9kB2LV99SWtFQYvSWvFQo/hKOBiukfYfAD4QF/+aLoHdM9eA7YzcNf+/Tkjj6f5PPD2/gHtH6iq2cdVSdKSeapR0lrxfWDPOWV7Ad8DngD8PfDrwIX9tVsBfreqDuxfd62qy/r5fjC7gKr6NPAI4Dt0z597+sDbIWkNM3hJWhOq6iZgc5JDAZLsBTwW+AywX1WdC7wE2APYHTgbeF6S9NM/YL7lJrkbcE1VvZXu4b8PHHpbJK1dnmqUtJY8Hfj7JG/oh18BfAs4N8kd6I5yHV9V1yf5S+AE4OI+fG0CnjjPMg8GXpzkZuCmfh2StCypWuiyCEmSJK0kTzVKkiQ1YvCSJElqxOAlSZLUiMFLkiSpEYOXJElSIwYvSZKkRgxekiRJjRi8JEmSGvn/c2UqpBEdZssAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.clf\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(np.asnumpy(np.arange(len(MBTIcheatList))),MBTIcheatList)\n",
    "plt.xlabel('Users')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mention of user MBTI type per user post')\n",
    "plt.ylim(0,40)\n",
    "plt.savefig('MBTImentionperuser.png',format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Reddit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total duplicated user: 206\n"
     ]
    }
   ],
   "source": [
    "reddit = pd.read_csv(reddit_path)\n",
    "print('total duplicated user:',(reddit['author'].duplicated(keep=False)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>entp</td>\n",
       "      <td>Yes! Personally I feel that as helpful as it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>intj</td>\n",
       "      <td>I guess I'd stick to being a lesbian so I woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>intp</td>\n",
       "      <td>What languages do you speak? Whoops, my bad fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>entp</td>\n",
       "      <td>It is just arguing semantics. To many on the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>entj</td>\n",
       "      <td>Come to Europe if you're interested. In many E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            comment\n",
       "0  entp  Yes! Personally I feel that as helpful as it i...\n",
       "1  intj  I guess I'd stick to being a lesbian so I woul...\n",
       "2  intp  What languages do you speak? Whoops, my bad fo...\n",
       "3  entp  It is just arguing semantics. To many on the c...\n",
       "4  entj  Come to Europe if you're interested. In many E..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_nodup=reddit[-reddit['author'].duplicated(keep=False)].reset_index(drop=True)[['type','comment']]\n",
    "# reddit_nodup.tocsv(reddit_nodup_path)\n",
    "reddit_nodup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the strings then save as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for Spacy\n",
      "loading spacy en_core_web_md\n",
      "finish loading\n"
     ]
    }
   ],
   "source": [
    "module1.useGPU(True)\n",
    "\n",
    "# ### load spacy\n",
    "nlp = module1.loadSpacy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file: mbti_1.csv\n",
      "start preprocessing\n",
      "finish preprocessing\n",
      "Tokenizing 1...\n",
      "Tokenizing 2...\n",
      "Tokenizing 3...\n",
      "Tokenizing 4...\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "for file in ['mbti_1.csv']:\n",
    "    print('processing file:', file)\n",
    "    \n",
    "    kaggle_path = os.path.join('Source Code','Raw','Kaggle',file)\n",
    "    kaggle = pd.read_csv(kaggle_path,header=0, names=['MBTI','post'])\n",
    "    print('start preprocessing')\n",
    "    kaggle_clean=module1.basicpreprocessing(kaggle,'post')\n",
    "    kaggle_label=module1.createLabel(kaggle,'MBTI')\n",
    "    print('finish preprocessing')\n",
    "    del kaggle\n",
    "    \n",
    "    \n",
    "    print('Tokenizing 1...')\n",
    "    kaggle_tokens=pd.Series([module1.spacy_tokenizer(post, nlp=nlp,rm_stop=False, filterPOS=['PUNCT']) for post in kaggle_clean['post'].tolist()],\n",
    "                            name='tokens') \n",
    "    \n",
    "    processed_path = os.path.join('Source Code','Processed','Kaggle','Kaggle-Filtered.pickle')\n",
    "    with open(processed_path, 'wb') as handle:\n",
    "        kaggle_DF=pd.concat([kaggle_label,kaggle_tokens],axis=1)\n",
    "        pickle.dump(kaggle_DF, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print('Tokenizing 2...')\n",
    "    kaggle_tokens=pd.Series([module1.spacy_tokenizer(post, nlp=nlp,rm_stop=True, filterPOS=['PUNCT','NOUN']) for post in kaggle_clean['post'].tolist()],\n",
    "                            name='tokens') \n",
    "    processed_path = os.path.join('Source Code','Processed','Kaggle','Kaggle-Filtered_noNNnoSW.pickle')\n",
    "    with open(processed_path, 'wb') as handle:\n",
    "        kaggle_DF=pd.concat([kaggle_label,kaggle_tokens],axis=1)\n",
    "        pickle.dump(kaggle_DF, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "    print('Tokenizing 3...')\n",
    "    kaggle_tokens=pd.Series([module1.spacy_tokenizer(post, nlp=nlp,rm_stop=False, filterPOS=['PUNCT','NOUN']) for post in kaggle_clean['post'].tolist()],\n",
    "                            name='tokens') \n",
    "    \n",
    "    processed_path = os.path.join('Source Code','Processed','Kaggle','Kaggle-Filtered_noNN.pickle')\n",
    "    with open(processed_path, 'wb') as handle:\n",
    "        kaggle_DF=pd.concat([kaggle_label,kaggle_tokens],axis=1)\n",
    "        pickle.dump(kaggle_DF, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print('Tokenizing 4...')\n",
    "    kaggle_tokens=pd.Series([module1.spacy_tokenizer(post, nlp=nlp,rm_stop=True, filterPOS=['PUNCT']) for post in kaggle_clean['post'].tolist()],\n",
    "                            name='tokens') \n",
    "    processed_path = os.path.join('Source Code','Processed','Kaggle','Kaggle-Filtered_noSW.pickle')\n",
    "    with open(processed_path, 'wb') as handle:\n",
    "        kaggle_DF=pd.concat([kaggle_label,kaggle_tokens],axis=1)\n",
    "        pickle.dump(kaggle_DF, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    del kaggle_tokens\n",
    "    \n",
    "    print('Done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove rows with less than 50 tokens from kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Source Code\\\\Processed\\\\Kaggle\\\\Kaggle-Filtered_noNNnoSW.pickle','rb') as pkl:\n",
    "        DF=pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Source Code\\\\Processed\\\\Kaggle\\\\Kaggle-Filtered_noNNnoSW.pickle','rb') as pkl:\n",
    "        DF=pickle.load(pkl)\n",
    "        \n",
    "below50index=DF['tokens'][(DF['tokens'].apply(lambda x: len(x))<50)].index.tolist()\n",
    "\n",
    "file_list=[]\n",
    "for root,subdir,files in os.walk(os.path.join('Source Code','Processed','Kaggle')):\n",
    "    file_list.extend([os.path.join(root,file) for file in files if file.endswith('.pickle')])\n",
    "\n",
    "         \n",
    "for file in file_list:         \n",
    "    with open('Source Code\\\\Processed\\\\Kaggle\\\\'+os.path.basename(file),'rb') as pkl:\n",
    "        DF=pickle.load(pkl)\n",
    "    DF=DF.drop(index=below50index).reset_index(drop=True)\n",
    "    with open('Source Code\\\\Processed\\\\Kaggle50\\\\'+os.path.basename(file),'wb') as pkl:\n",
    "        pickle.dump(DF, pkl, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Reddit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file: mbti9k_nodup.csv\n",
      "start preprocessing\n",
      "finish preprocessing\n"
     ]
    }
   ],
   "source": [
    "for file in ['mbti9k_nodup.csv']:\n",
    "    print('processing file:', file)\n",
    "    \n",
    "    reddit_path = os.path.join('Source Code','Raw','Reddit',file)\n",
    "    reddit = pd.read_csv(reddit_path,header=0, names=['MBTI','post'])\n",
    "    print('start preprocessing')\n",
    "    reddit_clean=module1.basicpreprocessing(reddit,'post')\n",
    "    reddit_label=module1.createLabel(reddit,'MBTI')\n",
    "    del reddit\n",
    "    print('finish preprocessing')\n",
    "    \n",
    "    print('Tokenizing 1...')\n",
    "    reddit_tokens=pd.Series([module1.spacy_tokenizer(post, nlp=nlp,rm_stop=False,filterPOS=['PUNCT']) for post in reddit_clean['post'].tolist()],\n",
    "                            name='tokens') \n",
    "    \n",
    "    processed_path = os.path.join('Source Code','Processed','Reddit','Reddit.pickle')\n",
    "    with open(processed_path, 'wb') as handle:\n",
    "        reddit_DF=pd.concat([reddit_label,reddit_tokens],axis=1)\n",
    "        pickle.dump(reddit_DF, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "    print('Tokenizing 2...')\n",
    "    reddit_tokens=pd.Series([module1.spacy_tokenizer(post, nlp=nlp,rm_stop=True, filterPOS=['PUNCT','NOUN']) for post in reddit_clean['post'].tolist()],\n",
    "                            name='tokens') \n",
    "    processed_path = os.path.join('Source Code','Processed','Reddit','Reddit_noNNnoSW.pickle')\n",
    "    with open(processed_path, 'wb') as handle:\n",
    "        reddit_DF=pd.concat([reddit_label,reddit_tokens],axis=1)\n",
    "        pickle.dump(reddit_DF, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        \n",
    "   \n",
    "    print('Tokenizing 3...')     \n",
    "    reddit_tokens=pd.Series([module1.spacy_tokenizer(post, nlp=nlp,rm_stop=False,filterPOS=['PUNCT','NOUN']) for post in reddit_clean['post'].tolist()],\n",
    "                            name='tokens') \n",
    "    \n",
    "    processed_path = os.path.join('Source Code','Processed','Reddit','Reddit_noNN.pickle')\n",
    "    with open(processed_path, 'wb') as handle:\n",
    "        reddit_DF=pd.concat([reddit_label,reddit_tokens],axis=1)\n",
    "        pickle.dump(reddit_DF, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "    print('Tokenizing 4...')\n",
    "    reddit_tokens=pd.Series([module1.spacy_tokenizer(post, nlp=nlp,rm_stop=True, filterPOS=['PUNCT']) for post in reddit_clean['post'].tolist()],\n",
    "                            name='tokens') \n",
    "    processed_path = os.path.join('Source Code','Processed','Reddit','Reddit_noSW.pickle')\n",
    "    with open(processed_path, 'wb') as handle:\n",
    "        reddit_DF=pd.concat([reddit_label,reddit_tokens],axis=1)\n",
    "        pickle.dump(reddit_DF, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    del reddit_tokens\n",
    "    \n",
    "    \n",
    "    print('done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Tokens to txt file as input for LIWC software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Source Code\\\\Processed\\\\Kaggle50\\\\kaggle-uncensored.pickle',\n",
       " 'Source Code\\\\Processed\\\\Kaggle50\\\\kaggle-uncensored_noNN.pickle',\n",
       " 'Source Code\\\\Processed\\\\Kaggle50\\\\kaggle-uncensored_noNNnoSW.pickle',\n",
       " 'Source Code\\\\Processed\\\\Kaggle50\\\\kaggle-uncensored_noSW.pickle',\n",
       " 'Source Code\\\\Processed\\\\Kaggle50\\\\kaggle.pickle',\n",
       " 'Source Code\\\\Processed\\\\Kaggle50\\\\kaggle_noNN.pickle',\n",
       " 'Source Code\\\\Processed\\\\Kaggle50\\\\kaggle_noNNnoSW.pickle',\n",
       " 'Source Code\\\\Processed\\\\Kaggle50\\\\kaggle_noSW.pickle']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list=[]\n",
    "for root,subdir,files in os.walk(os.path.join('Source Code','Processed','Kaggle50')):\n",
    "    file_list.extend([os.path.join(root,file) for file in files if file.endswith('.pickle')])\n",
    "# file_list=file_list[:3]\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in file_list:\n",
    "    with open(file,'rb') as pkl:\n",
    "        DF=pickle.load(pkl)\n",
    "    series_tokens=DF['tokens'].apply(lambda x: ' '.join(x))\n",
    "    for i,row in enumerate(series_tokens):\n",
    "        path=os.path.join('Source Code','LIWC',os.path.basename(file).replace('.pickle',''),f'row_{str(i).zfill(4)}.txt')\n",
    "        if not os.path.isdir(os.path.split(path)[0]):\n",
    "            os.mkdir(os.path.split(path)[0])\n",
    "        with open(path,'w+', encoding='utf-8') as txt:\n",
    "            txt.write(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a converted LIWC csv for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileLIWCpath=os.path.join('Source Code','Processed','LIWC','LIWC_'+os.path.basename(file_list[0]).replace('.pickle','.csv'))\n",
    "fileLIWC=pd.read_csv(fileLIWCpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>Tone</th>\n",
       "      <th>Sixltr</th>\n",
       "      <th>function</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ppron</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>...</th>\n",
       "      <th>home</th>\n",
       "      <th>money</th>\n",
       "      <th>relig</th>\n",
       "      <th>death</th>\n",
       "      <th>informal</th>\n",
       "      <th>swear</th>\n",
       "      <th>netspeak</th>\n",
       "      <th>assent</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.161323</td>\n",
       "      <td>0.135104</td>\n",
       "      <td>0.117713</td>\n",
       "      <td>0.116484</td>\n",
       "      <td>0.029660</td>\n",
       "      <td>0.095766</td>\n",
       "      <td>0.024291</td>\n",
       "      <td>0.015369</td>\n",
       "      <td>0.007146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.041871</td>\n",
       "      <td>0.064939</td>\n",
       "      <td>0.040848</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.048899</td>\n",
       "      <td>0.016809</td>\n",
       "      <td>0.012911</td>\n",
       "      <td>0.008636</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053117</td>\n",
       "      <td>0.074417</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>0.088098</td>\n",
       "      <td>0.018221</td>\n",
       "      <td>0.065190</td>\n",
       "      <td>0.020319</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.041499</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.057983</td>\n",
       "      <td>0.054390</td>\n",
       "      <td>0.013746</td>\n",
       "      <td>0.055178</td>\n",
       "      <td>0.018088</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>0.006647</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.043960</td>\n",
       "      <td>0.068896</td>\n",
       "      <td>0.027314</td>\n",
       "      <td>0.048047</td>\n",
       "      <td>0.014650</td>\n",
       "      <td>0.059841</td>\n",
       "      <td>0.017463</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8632</td>\n",
       "      <td>0.071334</td>\n",
       "      <td>0.076420</td>\n",
       "      <td>0.055707</td>\n",
       "      <td>0.059670</td>\n",
       "      <td>0.017279</td>\n",
       "      <td>0.073289</td>\n",
       "      <td>0.020766</td>\n",
       "      <td>0.013435</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8633</td>\n",
       "      <td>0.018573</td>\n",
       "      <td>0.025947</td>\n",
       "      <td>0.062069</td>\n",
       "      <td>0.074664</td>\n",
       "      <td>0.010198</td>\n",
       "      <td>0.044405</td>\n",
       "      <td>0.014336</td>\n",
       "      <td>0.010198</td>\n",
       "      <td>0.007107</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8634</td>\n",
       "      <td>0.058323</td>\n",
       "      <td>0.082522</td>\n",
       "      <td>0.042532</td>\n",
       "      <td>0.049416</td>\n",
       "      <td>0.015108</td>\n",
       "      <td>0.058680</td>\n",
       "      <td>0.018268</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8635</td>\n",
       "      <td>0.021267</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>0.050503</td>\n",
       "      <td>0.041652</td>\n",
       "      <td>0.007573</td>\n",
       "      <td>0.035512</td>\n",
       "      <td>0.011285</td>\n",
       "      <td>0.007395</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8636</td>\n",
       "      <td>0.018568</td>\n",
       "      <td>0.020629</td>\n",
       "      <td>0.070303</td>\n",
       "      <td>0.052015</td>\n",
       "      <td>0.007803</td>\n",
       "      <td>0.046083</td>\n",
       "      <td>0.015091</td>\n",
       "      <td>0.010447</td>\n",
       "      <td>0.007750</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8637 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Analytic     Clout  Authentic      Tone    Sixltr  function   pronoun  \\\n",
       "0     0.161323  0.135104   0.117713  0.116484  0.029660  0.095766  0.024291   \n",
       "1     0.048400  0.041871   0.064939  0.040848  0.009091  0.048899  0.016809   \n",
       "2     0.053117  0.074417   0.064209  0.088098  0.018221  0.065190  0.020319   \n",
       "3     0.041499  0.053900   0.057983  0.054390  0.013746  0.055178  0.018088   \n",
       "4     0.043960  0.068896   0.027314  0.048047  0.014650  0.059841  0.017463   \n",
       "...        ...       ...        ...       ...       ...       ...       ...   \n",
       "8632  0.071334  0.076420   0.055707  0.059670  0.017279  0.073289  0.020766   \n",
       "8633  0.018573  0.025947   0.062069  0.074664  0.010198  0.044405  0.014336   \n",
       "8634  0.058323  0.082522   0.042532  0.049416  0.015108  0.058680  0.018268   \n",
       "8635  0.021267  0.015234   0.050503  0.041652  0.007573  0.035512  0.011285   \n",
       "8636  0.018568  0.020629   0.070303  0.052015  0.007803  0.046083  0.015091   \n",
       "\n",
       "         ppron         i        we  ...      home     money     relig  \\\n",
       "0     0.015369  0.007146  0.000000  ...  0.001437  0.000000  0.000000   \n",
       "1     0.012911  0.008636  0.000455  ...  0.000000  0.000000  0.000227   \n",
       "2     0.009791  0.005423  0.000307  ...  0.000147  0.000000  0.000454   \n",
       "3     0.012459  0.006647  0.001201  ...  0.000183  0.000000  0.000365   \n",
       "4     0.010372  0.003949  0.000446  ...  0.000117  0.000679  0.000117   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8632  0.013435  0.006803  0.000000  ...  0.000000  0.001044  0.000700   \n",
       "8633  0.010198  0.007107  0.000405  ...  0.000351  0.000176  0.000061   \n",
       "8634  0.012186  0.004448  0.000942  ...  0.000346  0.000238  0.000238   \n",
       "8635  0.007395  0.005435  0.000178  ...  0.000036  0.000243  0.000036   \n",
       "8636  0.010447  0.007750  0.000174  ...  0.000227  0.000114  0.000114   \n",
       "\n",
       "         death  informal     swear  netspeak    assent    nonflu    filler  \n",
       "0     0.000718  0.002136  0.000000  0.001437  0.000359  0.000359  0.000000  \n",
       "1     0.000227  0.000385  0.000079  0.000149  0.000149  0.000000  0.000000  \n",
       "2     0.000000  0.001055  0.000000  0.000147  0.000454  0.000147  0.000307  \n",
       "3     0.000000  0.001287  0.000279  0.000826  0.000096  0.000096  0.000000  \n",
       "4     0.000117  0.001242  0.000786  0.000340  0.000000  0.000117  0.000000  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8632  0.000528  0.002444  0.001044  0.000172  0.000528  0.000172  0.000528  \n",
       "8633  0.000000  0.000878  0.000061  0.000290  0.000290  0.000290  0.000000  \n",
       "8634  0.000119  0.000823  0.000346  0.000238  0.000000  0.000238  0.000000  \n",
       "8635  0.000000  0.000243  0.000071  0.000000  0.000142  0.000036  0.000000  \n",
       "8636  0.000061  0.000689  0.000000  0.000061  0.000174  0.000288  0.000174  \n",
       "\n",
       "[8637 rows x 78 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileLIWC=fileLIWC.iloc[:,3:].div(fileLIWC.iloc[:,2].tolist(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
