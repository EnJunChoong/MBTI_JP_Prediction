{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import module1_Preprocessing as module1\n",
    "import module2_FeatureExtraction as module2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score , roc_auc_score, confusion_matrix\n",
    "\n",
    "os.chdir('../')\n",
    "LIWCPath=os.path.join('Processed','LIWC')\n",
    "ResultDump='FinalResults'\n",
    "seed=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Processed\\\\Kaggle50\\\\Kaggle-Filtered.pickle',\n",
       " 'Processed\\\\Kaggle50\\\\Kaggle-Filtered_noNN.pickle',\n",
       " 'Processed\\\\Kaggle50\\\\Kaggle-Filtered_noNNnoSW.pickle',\n",
       " 'Processed\\\\Kaggle50\\\\Kaggle-Filtered_noSW.pickle',\n",
       " 'Processed\\\\Kaggle50\\\\Kaggle.pickle',\n",
       " 'Processed\\\\Kaggle50\\\\Kaggle_noNN.pickle',\n",
       " 'Processed\\\\Kaggle50\\\\Kaggle_noNNnoSW.pickle',\n",
       " 'Processed\\\\Kaggle50\\\\Kaggle_noSW.pickle']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list=[]\n",
    "for root,subdir,files in os.walk(os.path.join('Processed','Kaggle50')):\n",
    "    file_list.extend([os.path.join(root,file) for file in files if file.endswith('.pickle')])\n",
    "file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "cnb = ComplementNB()\n",
    "cnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=100, n_jobs=10, oob_score=False,\n",
       "                       random_state=123, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier(n_estimators=100,random_state=123, class_weight='balanced', n_jobs=10 )\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=123, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr=LogisticRegression(random_state=123, solver= 'liblinear',class_weight='balanced')\n",
    "lgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=123, shrinking=True, tol=0.001,\n",
       "    verbose=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC(random_state=seed, verbose=True, C = 1.0, class_weight='balanced', probability=True, gamma='auto')\n",
    "svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight='balanced',\n",
       "               colsample_bytree=1.0, importance_type='split',\n",
       "               learning_rate=0.01, max_depth=-1, metric='auc,binary_logloss',\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=10000, n_jobs=10, num_leaves=31, objective='binary',\n",
       "               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "lgb = lgb.LGBMClassifier(objective='binary',metric='auc,binary_logloss',boosting_type='gbdt', class_weight='balanced',\n",
    "                               learning_rate=0.01,n_estimators=10000,\n",
    "                              random_state=seed,n_jobs=10)\n",
    "lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with 5-folds Cross Validation. Loop for 4 different models and different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer, quantile_transform\n",
    "\n",
    "scaler=QuantileTransformer(random_state=seed) \n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = seed)\n",
    "result_list=[]\n",
    "all_result_list=[]\n",
    "result_fileDF=pd.DataFrame()\n",
    "models=[('lgb',lgb),('cnb',cnb),('rf',rf),('lgr',lgr),('svc',svc)]\n",
    "\n",
    "for filepath in file_list:\n",
    "    result_list=[]\n",
    "    dir = os.path.split(filepath)[0]\n",
    "    file = os.path.split(filepath)[1].replace('.pickle','')\n",
    "    print('processing file:',file)\n",
    "    \n",
    "    with open(filepath, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    \n",
    "        label=data[['MBTI','EI','SN','TF','JP']]\n",
    "    \n",
    "    data['tokens']=data['tokens'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('converting to tfidf')\n",
    "    cCount_Vect=CountVectorizer(analyzer='char_wb', ngram_range=(2,3),min_df=50, max_df=0.95, max_features=1500)\n",
    "    wCount_Vect=CountVectorizer(tokenizer = module2.dummy, preprocessor=module2.dummy, token_pattern=module2.dummy,\n",
    "                           analyzer='char_wb', ngram_range=(2,3),min_df=50, max_df=0.95, max_features=1500)\n",
    "    TFIDF_Trans=TfidfTransformer(use_idf=True, sublinear_tf=False)\n",
    "    cTfidf_Vect=TfidfVectorizer(analyzer='char_wb', ngram_range=(2,3),min_df=50, max_df=0.95, max_features=1500)\n",
    "    wTfidf_Vect=TfidfVectorizer(tokenizer = module2.dummy, preprocessor=module2.dummy, token_pattern=module2.dummy, \n",
    "                                ngram_range=(1,3),min_df=50, max_df=0.95, max_features=1500)\n",
    "    \n",
    "    char_tf, char_tf_top=module2.df2vector(data['tokens'], cCount_Vect, 1500)\n",
    "    word_tf, word_tf_top=module2.df2vector(data['tokens'], wCount_Vect, 1500)\n",
    "    \n",
    "    char_tfidf, char_tfidf_top=module2.df2vector(data['tokens'], cTfidf_Vect, 1500)\n",
    "    word_tfidf, word_tfidf_top=module2.df2vector(data['tokens'], wTfidf_Vect, 1500)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    del data\n",
    "    \n",
    "    LIWC=module2.LIWC(LIWCPath,filepath)\n",
    "\n",
    "    combo = np.hstack([LIWC,word_tfidf, char_tfidf, word_tf, char_tf])    \n",
    "    char_tf_col=char_tf_top[0].tolist()\n",
    "    word_tf_col=word_tf_top[0].tolist()\n",
    "    char_tfidf_col=char_tfidf_top[0].tolist()\n",
    "    word_tfidf_col=word_tfidf_top[0].tolist()\n",
    "    LIWC_col=LIWC.columns.tolist()\n",
    "    \n",
    "    char_tf=scaler.fit_transform(char_tf)\n",
    "    word_tf=scaler.fit_transform(word_tf)\n",
    "    char_tfidf=scaler.fit_transform(char_tfidf)\n",
    "    word_tfidf=scaler.fit_transform(word_tfidf)\n",
    "    LIWC=scaler.fit_transform(LIWC)\n",
    "    \n",
    "\n",
    "    \n",
    "    features = [('combo',combo),('char_tf',char_tf), ('word_tf',word_tf), ('char_tfidf',char_tfidf), ('word_tfidf',word_tfidf),('LIWC',LIWC)]\n",
    "    for feature_name,feature in features:    \n",
    "        X=feature\n",
    "        feature_col=[]\n",
    "        if feature_name =='combo':\n",
    "            feature_col=LIWC_col\n",
    "            feature_col.extend(word_tfidf_col)\n",
    "            feature_col.extend(char_tfidf_col)\n",
    "            feature_col.extend(word_tf_col)\n",
    "            feature_col.extend(char_tf_col)\n",
    "        elif feature_name =='char_tfidf':\n",
    "            feature_col =char_tfidf_col\n",
    "        elif feature_name =='word_tfidf':\n",
    "            feature_col =word_tfidf_col      \n",
    "        elif feature_name =='LIWC':\n",
    "            feature_col = LIWC_col\n",
    "        \n",
    "        \n",
    "        for y_class in ['JP']:\n",
    "            y=label[y_class]\n",
    "            split = 0\n",
    "            print('\\n\\n\\nPerforming training and classification on:',file, feature_name, y_class)\n",
    "\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                \n",
    "                split += 1\n",
    "                X_train, X_test = X[train_index,:],X[test_index,:]\n",
    "                y_train, y_test = y[train_index],y[test_index]\n",
    "\n",
    "                print('\\nNow on split#',split, feature_name, y_class )\n",
    "                for model_name, model in models:\n",
    "                    result = {}\n",
    "                    start_time = time.time()\n",
    "                    if model_name == 'lgb':\n",
    "                        lgb.fit(X_train,y_train,eval_set=[(X_test,y_test)],eval_metric='f1_score',\n",
    "                                verbose=100,early_stopping_rounds=300)\n",
    "                        y_pred =lgb.predict(X_test)\n",
    "                        y_pred_proba = lgb.predict_proba(X_test)[:,1]\n",
    "                        \n",
    "                        # sorted(zip(clf.feature_importances_, X.columns), reverse=True)\n",
    "                        feature_imp = pd.DataFrame(sorted(zip(lgb.feature_importances_,feature_col)), columns=['Value','Feature'])\n",
    "\n",
    "                        plt.figure(figsize=(20, 10))\n",
    "                        sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[:20])\n",
    "                        plt.title('LightGBM Features (avg over folds)')\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                        plt.savefig(filepath.replace('.pickle',f'_{feature_name}_lgb_importances.png'), format='png')\n",
    "                        with open(filepath.replace('.pickle','_lgbfeatures.pickle'), 'wb') as handle:\n",
    "                            pickle.dump(feature_imp, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    else:\n",
    "                        model.fit(X_train,y_train)\n",
    "                        y_pred =model.predict(X_test)\n",
    "                        y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "                    f1=f1_score(y_test, y_pred, average='macro') \n",
    "                    accuracy=accuracy_score(y_test, y_pred)\n",
    "                    cm=confusion_matrix(y_test, y_pred)\n",
    "                    auc=roc_auc_score(y_test, y_pred_proba)\n",
    "                    seconds=time.time() - start_time\n",
    "                    print(model_name, accuracy, f1,auc)\n",
    "                    result['file'] = file\n",
    "                    result['cv_split#'] = split\n",
    "                    result['class'] = y_class\n",
    "                    result['Feature'] = feature_name\n",
    "                    result['model'] = model_name\n",
    "                    result['CM'] = cm\n",
    "                    result['Acc'] = accuracy\n",
    "                    result['f1-macro'] = f1\n",
    "                    result['auc'] = auc\n",
    "                    result['time-s'] = seconds\n",
    "                    result_list.append(result)\n",
    "                    print(\"--- %s seconds ---\" %seconds)\n",
    "    all_result_list.extend(result_list)\n",
    "    result_fileDF = pd.DataFrame(result_list)\n",
    "    result_fileDF.to_csv(os.path.join(ResultDummp,file+'.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
